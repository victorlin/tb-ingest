#!/usr/bin/env bash
# Makes tb-profiler and snippy result files available for a sample, either by
# downloading from S3 or running the tools and uploading to S3 so they can be
# downloaded in the future.

set -euo pipefail


sra_id=$1
# sra_id=SRR19320509

cpus=1

s3_bucket="fh-pi-bedford-t-eco-public"


main() {
    check-aws
    get-fastq
    get-tb-profiler-files
    get-snippy-files
}


check-aws() {
    if ! command -v aws &> /dev/null; then
        echo "Error: AWS CLI is not available." >&2
        exit 1
    fi

    if ! aws s3 ls "s3://${s3_bucket}" > /dev/null 2>&1; then
        echo "Error: Unable to access s3://${s3_bucket}." >&2
        exit 1
    fi
}


fastq_dir="data/fastq"
mkdir -p "${fastq_dir}"

# For all samples
fastq1="${fastq_dir}/${sra_id}_1.fastq"
# For paired-end samples
fastq2="${fastq_dir}/${sra_id}_2.fastq"


get-fastq() {
    echo "Downloading SRA file…" >&2
    # This is basically `prefetch $sra_id` but optimized for running on AWS (ideally in us-east-1).
    # FIXME: Download fastq directly from s3://sra-pub-src-1/ or s3://sra-pub-src-2/ if possible, to avoid fastq-dump.
    # Maybe not possible since we are filtering to Illumina which is not listed on <https://registry.opendata.aws/ncbi-sra/>.
    aws s3 cp --no-sign-request "s3://sra-pub-run-odp/sra/${sra_id}/${sra_id}" "${sra_id}.sra"

    echo "Dumping FASTQ…" >&2
    fasterq-dump --split-files --outdir "${fastq_dir}" "${sra_id}.sra"

    if [[ ! -f "${fastq1}" ]]; then
        echo "Error: FASTQ file _1.fastq not found for ${sra_id}" >&2
        exit 1
    fi
}


get-tb-profiler-files() {
    # Download files if available on S3, otherwise run tb-profiler and upload files.
    tb_profiler_out="data/tbprofiler"
    mkdir -p "${tb_profiler_out}"

    local_path="${tb_profiler_out}/results/${sra_id}.results.json"
    s3_path="files/workflows/tb/${local_path}"

    echo "Running tb-profiler…" >&2

    if [[ -f "${fastq2}" ]]; then
        tb-profiler profile \
            --threads "${cpus}" \
            -1 "${fastq1}" \
            -2 "${fastq2}" \
            -p "${sra_id}" \
            --txt \
            --dir "${tb_profiler_out}"
    else
        tb-profiler profile \
            --threads "${cpus}" \
            -1 "${fastq1}" \
            -p "${sra_id}" \
            --txt \
            --dir "${tb_profiler_out}"
    fi

    echo "Uploading results to S3…" >&2
    aws s3 cp "${local_path}" "s3://${s3_bucket}/${s3_path}"
}


get-snippy-files() {
    # Download files if available on S3, otherwise run snippy and upload files.
    snippy_out="data/snippy/${sra_id}"
    mkdir -p "${snippy_out}"

    local_path="${snippy_out}"
    s3_path="files/workflows/tb/${local_path}"

    echo "Running snippy…" >&2

    reference="defaults/GCF_000195955.2_ASM19595v2_genomic.gbff"

    if [[ -f "${fastq2}" ]]; then
        snippy \
            --cpus "${cpus}" \
            --outdir "${snippy_out}" \
            --R1 "${fastq1}" \
            --R2 "${fastq2}" \
            --ref "${reference}" \
            --force
    else
        snippy \
            --cpus "${cpus}" \
            --outdir "${snippy_out}" \
            --se "${fastq1}" \
            --ref "${reference}" \
            --force
    fi

    echo "Uploading results to S3…" >&2
    aws s3 cp --recursive "${local_path}" "s3://${s3_bucket}/${s3_path}"
}

main
